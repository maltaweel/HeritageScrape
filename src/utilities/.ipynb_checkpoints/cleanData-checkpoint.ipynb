{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-37540add3598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;31m#calls the run method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-37540add3598>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m#clean data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mcleanData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-37540add3598>\u001b[0m in \u001b[0;36mcleanData\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcleanData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#get the patheay to the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mpn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mpn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"src\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Module used to clean Twitter data.\n",
    "\n",
    "Created on Jun 18, 2020\n",
    "\n",
    "@author: mark\n",
    "'''\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "import csv\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "#English stop words\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "'''\n",
    "Main method to remove stop and short (2-letter words or less) words, with the cleaning of text by removing symbols and non-language text.\n",
    "'''\n",
    "def cleanData():\n",
    "    #get the patheay to the data\n",
    "    pn=os.path.abspath(\"\")\n",
    "    pn=pn.split(\"src\")[0]\n",
    "    \n",
    "    #the data directory\n",
    "    directory=os.path.join(pn,'results')\n",
    "    \n",
    "    #output directory for cleaned file\n",
    "    output_directory=os.path.join(pn,'modified')\n",
    "    \n",
    "    #pattern for removing text\n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "                        \n",
    "    files={}\n",
    "    \n",
    "    #now go through file(s) and begin cleaning text\n",
    "    try:\n",
    "        for f in listdir(directory):\n",
    "            rows=[]\n",
    "            \n",
    "            #open files\n",
    "            with open(os.path.join(directory,f),'r') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "            \n",
    "                #go through the rows\n",
    "                for row in reader:\n",
    "                    \n",
    "                    #get text\n",
    "                    text=row['Text']\n",
    "                    \n",
    "                    #make text lower case \n",
    "                    text=text.lower()\n",
    "                    \n",
    "                    #initial processing \n",
    "                    text=re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', text)\n",
    "                    text=pattern.sub(\"\",text)\n",
    "                    \n",
    "                    #tokenize text\n",
    "                    word_tokens = word_tokenize(text) \n",
    "                    \n",
    "                    #block of filters to clean sentences\n",
    "                    #removes stop words, @ symbol, http, and various punctuation listed\n",
    "                    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "                    filtered_sentence = [w for w in filtered_sentence if not w.startswith(\"@\")]\n",
    "                    filtered_sentence = [w for w in filtered_sentence if not w.startswith(\"http\")]\n",
    "                    filtered_sentence = [w for w in filtered_sentence if \" amp \" not in w]\n",
    "                    \n",
    "                    #removal of commas, punctuations, brackets, \n",
    "                    words = [w.replace('(', '') for w in filtered_sentence]\n",
    "                    words = [w.replace(')', '') for w in words]\n",
    "                    words = [w.replace('?', '') for w in words]\n",
    "                    words = [w.replace(',', '') for w in words]\n",
    "                    words = [w.replace(\"'\", '') for w in words]\n",
    "                    words = [w.replace('\"', '') for w in words]\n",
    "                    words = [w.replace('!', '') for w in words]\n",
    "                    words = [w.replace(':', '') for w in words]\n",
    "                    words = [w.replace('&amp;', '') for w in words]\n",
    "                    words = [w.replace('.', '') for w in words]\n",
    "                    words = [w.replace('/', '') for w in words]\n",
    "                    words = [w.replace('[', '') for w in words]\n",
    "                    words = [w.replace(']', '') for w in words] \n",
    "                    words = [w for w in words if len(w) > 2]\n",
    "                    \n",
    "                    #re-add cleaned words to tweet text    \n",
    "                    w = \" \".join(words)\n",
    "\n",
    "                    #add text to the dictionary and then the row\n",
    "                    row['Text']=w\n",
    "                    rows.append(row)\n",
    "                \n",
    "                #call the output of the clean data\n",
    "                fle=os.path.join(output_directory,'modified'+\"_\"+f)   \n",
    "                output(rows,fle)  \n",
    "                  \n",
    "            files[f]=rows      \n",
    "    \n",
    "    #exception handling\n",
    "    except IOError:\n",
    "        print (\"Could not read file:\", csvfile)\n",
    "\n",
    "'''\n",
    "Method to output the cleaned text data.\n",
    "@param fileOutput- the file output directory\n",
    "'''\n",
    "def output(data,fileOutput):\n",
    "    #the fieldnames or title of columns for the output file\n",
    "    fieldnames = ['Datetime','ID','Link','Text','Username','Retweets','Hashtags','Geolocation']\n",
    "    \n",
    "    #output file\n",
    "    with open(fileOutput, 'wt') as csvf:\n",
    "        writer = csv.DictWriter(csvf, fieldnames=fieldnames)\n",
    "\n",
    "        #write the header\n",
    "        writer.writeheader()  \n",
    "        \n",
    "        #go through the data and print data with cleaned text\n",
    "        for f in data:\n",
    "            writer.writerow({'Datetime': str(f['Datetime']),\n",
    "                             'ID':str(f['ID']),'Link':str(f['Link']),\n",
    "                             'Text':str(f['Text']),'Username':str(f['Username']),'Retweets':str(f['Retweets']),'Hashtags':str(f['Hashtags']),\n",
    "                              'Geolocation':str(f['Geolocation'])})\n",
    "    \n",
    "'''\n",
    "The main run method to run the cleanup module.\n",
    "'''        \n",
    "def run():\n",
    "    #clean data\n",
    "    cleanData()\n",
    "    \n",
    "    print('Finished')\n",
    "\n",
    "#calls the run method\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
