{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The main module used for topic modelling.\n",
    "Topic models include latent Dirichlet allocation (LDA) and Hierarchical Dirichlet Process (HDP).\n",
    "\n",
    "Created on Jul 1, 2020\n",
    "\n",
    "@author: mark\n",
    "'''\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "import datetime\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer as word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import CoherenceModel, LdaModel, HdpModel\n",
    "\n",
    "import re\n",
    "import pyLDAvis.gensim\n",
    "import gensim\n",
    "from gensim.utils import lemmatize\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "pn=os.path.abspath(__file__)\n",
    "pn=pn.split(\"src\")[0]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "class TopicModel():\n",
    "    \n",
    "    '''\n",
    "    Simple method to prepocess text using gensim\n",
    "    '''\n",
    "    def first_process(self,texts):\n",
    "\n",
    "        for t in texts:\n",
    "            t=yield gensim.utils.simple_preprocess(t, deacc=True, min_len=3)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Method to process text and lemmatize.\n",
    "    \n",
    "    @param fullt_text- the full text to create bigram\n",
    "    @param texts- texts to analyze for topic models and process using lemmatization.\n",
    "    \n",
    "    @return corpus- the corpus of text to analyze\n",
    "    @return dictionary- the term dictionary to match against\n",
    "    '''       \n",
    "    def process_text(self, full_text, texts):\n",
    "        \n",
    "        #develop bigram\n",
    "        bigram = gensim.models.Phrases(full_text, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "        bigram = gensim.models.phrases.Phraser(bigram)\n",
    " #      trigram = gensim.tsmodels.phrases.Phraser(full_text, threshold=100)\n",
    "        \n",
    "        #get texts from bigram\n",
    "        texts = [bigram[line] for line in texts]\n",
    "  #     texts = [[word.split('/')[0] for word in lemmatize(' '.join(line), allowed_tags=re.compile('(NN)'), min_length=3)] for line in texts]\n",
    "  #      texts_lemma = ' '.join([lemmatizer.lemmatize(w) for w in texts])\n",
    "        \n",
    "        #get lemmaztized words\n",
    "        texts_lemma=[]\n",
    "        for line in texts:\n",
    "            for word in line:\n",
    "                l=lemmatizer.lemmatize(word)\n",
    "                texts_lemma.append(l)\n",
    "        \n",
    "        #create dictionary\n",
    "        texts_lemma = [d.split() for d in texts_lemma]\n",
    "        dictionary = Dictionary(texts_lemma)\n",
    "        \n",
    "        #create the corpus\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts_lemma]\n",
    "        \n",
    "        return corpus, dictionary\n",
    "        \n",
    "    '''\n",
    "    Method to load text data based on a start and end data.\n",
    "    \n",
    "    @param start- the start date to load text\n",
    "    @param end- the end date to load text\n",
    "    '''  \n",
    "    def loadData(self,start,end):\n",
    "        \n",
    "        #the pathway to the data to analyze\n",
    "        pn=os.path.abspath(__file__)\n",
    "        pn=pn.split(\"src\")[0]  \n",
    "        directory=os.path.join(pn,'modified')\n",
    "        \n",
    "        rows=[]\n",
    "        \n",
    "        #get the text file from the directory\n",
    "        try:\n",
    "            for f in listdir(directory):\n",
    "                \n",
    "                \n",
    "                if '.csv' not in f:\n",
    "                    continue\n",
    "                \n",
    "                #open the text\n",
    "                with open(os.path.join(directory,f),'rt') as csvfile:\n",
    "                    \n",
    "                    #get the reader\n",
    "                    reader = csv.DictReader(csvfile)\n",
    "            \n",
    "                    #get single reader\n",
    "                    for row in reader:\n",
    "                        \n",
    "                        #get text\n",
    "                        text=row['Text'] #.decode('utf-8')\n",
    "                        \n",
    "                        #get date\n",
    "                        date=row['Datetime'].split(\" \")[0]\n",
    "                        \n",
    "                        #get dates (start and end)\n",
    "                        date_time_obj = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "                        start_d=datetime.datetime.strptime(start, '%Y-%m-%d')\n",
    "                        end_d=datetime.datetime.strptime(end, '%Y-%m-%d')\n",
    "                        \n",
    "                        date=date_time_obj.date()\n",
    "                        startD=start_d.date()\n",
    "                        endD=end_d.date()\n",
    "                        \n",
    "                        if date>=startD:\n",
    "                            if date<endD:\n",
    "                                rows.append(text)       \n",
    "                        \n",
    "        # the exception handling           \n",
    "        except IOError:\n",
    "            print (\"Could not read file:\", csvfile)               \n",
    "        \n",
    "        return rows\n",
    "    \n",
    "    \"\"\"\n",
    "    Method for using a coherence model to look at topic coherence for LDA models.\n",
    "    \n",
    "    @param dictionary- Gensim dictionary\n",
    "    @param corpus- Gensim corpus\n",
    "    @param limit- topic limit\n",
    "    \n",
    "    @return lm_list- List of LDA topic models\n",
    "    @return c_v- Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    def evaluate_graph(self, dictionary, corpus, texts, limit):\n",
    "    \n",
    "        c_v = []\n",
    "        lm_list = []\n",
    "        for num_topics in range(1, (limit*2)+1):\n",
    "            lm = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,alpha=\"auto\")\n",
    "            lm_list.append(lm)\n",
    "            cm = CoherenceModel(model=lm, texts=[texts], corpus=corpus, coherence='u_mass')\n",
    "            c_v.append(cm.get_coherence())\n",
    "            del cm\n",
    "            \n",
    "        return lm_list, c_v\n",
    "\n",
    "    '''\n",
    "    Method to print csv output results of the evaluations conducted \n",
    "    \n",
    "    @param modList- the model evaluated\n",
    "    @param results- the result scores\n",
    "    @param i- the index output desired\n",
    "    @param start- the start date\n",
    "    @param end- the end date\n",
    "    '''\n",
    "    def printEvaluation(self,modList,results,i,start,end):\n",
    "       \n",
    "        filename=os.path.join(pn,'topic_model_results','evaluationTotal'+str(i)+\"_\"+start+\"_\"+end+\".csv\")   \n",
    "        \n",
    "        fieldnames = ['Model','Score']\n",
    "    \n",
    "        with open(filename, 'w') as csvf:\n",
    "                writer = csv.DictWriter(csvf, fieldnames=fieldnames)\n",
    "\n",
    "                writer.writeheader()\n",
    "                for t in range(0,len(modList)):\n",
    "        \n",
    "                    writer.writerow({'Model':str(modList[t]),'Score': str(results[t])})\n",
    "                \n",
    "    '''\n",
    "    Method to output results of the analysis.\n",
    "    \n",
    "    @param i- the topic number\n",
    "    @param results- the results from the model\n",
    "    @param model- the model used (e.g., lda, hdp) to output\n",
    "    @param start- the start date\n",
    "    @param end- the end date\n",
    "    '''\n",
    "    def printResults(self,i,results,model,start,end):\n",
    "\n",
    "        filename=os.path.join(pn,'topic_model_results','analysis_results_'+model+str(i)+\"_\"+start+\"_\"+\n",
    "                              end+\".csv\")\n",
    "        \n",
    "        fieldnames = ['Topic','Term','Value']\n",
    "\n",
    "        with open(filename, 'w') as csvf:\n",
    "            writer = csv.DictWriter(csvf, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for l in results:\n",
    "                n=l[0]\n",
    "                v=l[1]\n",
    "                vvs=v.split(\"+\")\n",
    "                for vv in vvs:\n",
    "                    vvt=vv.split(\"*\")\n",
    "                    if len(vvt)<2:\n",
    "                        continue\n",
    "                    t=vvt[1]\n",
    "                    val=vvt[0]\n",
    "                    writer.writerow({'Topic':str(n),'Term': str(t.encode(\"utf-8\")),'Value':str(val)})\n",
    "    \n",
    "    '''\n",
    "    Method to run the topic models (lda and hdp).\n",
    "    @param number_of_topics- the number of topics\n",
    "    @param corpus- the corpus of text\n",
    "    @param dictionary- the dictionary of terms\n",
    "    @param start- the start date\n",
    "    @param end- the end date\n",
    "    '''\n",
    "    def runModels(self,number_of_topics, corpus, dictionary,start,end):\n",
    "        \n",
    "        #do hdp model\n",
    "        hdpmodel = HdpModel(corpus=corpus, id2word=dictionary)\n",
    "        \n",
    "        hdpmodel.print_topics(num_topics=int(number_of_topics), num_words=10)\n",
    "        hdptopics = hdpmodel.show_topics(num_topics=int(number_of_topics))\n",
    "\n",
    "    #   result_dict=addTotalTermResults(hdptopics)\n",
    "            \n",
    "        #add results to total kept in a list     \n",
    "    #   addToResults(result_dict)\n",
    "    \n",
    "        #output results\n",
    "        self.printResults(number_of_topics,hdptopics,'hdp',start,end)\n",
    "        \n",
    "        #d lda model\n",
    "        ldamodel = LdaModel(corpus=corpus, num_topics=number_of_topics, id2word=dictionary,passes=20,iterations=400)\n",
    "       \n",
    "        ldamodel.save('lda'+number_of_topics+'.model')\n",
    "        ldatopics = ldamodel.show_topics(num_topics=int(number_of_topics))\n",
    "    \n",
    "    #   result_dict=addTotalTermResults(ldatopics)    \n",
    "    #   addToResults(result_dict)\n",
    "        self.printResults(number_of_topics,ldatopics,'lda',start,end)\n",
    "    \n",
    "    \n",
    "        visualisation = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "   \n",
    "        location=os.path.join(pn,'topic_model_results')\n",
    "     \n",
    "        #visualize outputs in html\n",
    "        pyLDAvis.save_html(visualisation, os.path.join(location,'LDA_Visualization'+str(number_of_topics)+\"_\"+start+\n",
    "                                                        \"_\"+end+'.html')) \n",
    "        \n",
    "        \n",
    "        \n",
    "'''\n",
    "Method to run the module.\n",
    "\n",
    "@param argv- the runtime argument\n",
    "'''           \n",
    "def run(argv):\n",
    "    \n",
    "    #get run arguments\n",
    "    tm=TopicModel()\n",
    "    number_of_topics = argv[1]\n",
    "    start=argv[2]\n",
    "    end=argv[3]\n",
    "    \n",
    "    #load and process text\n",
    "    original_texts=tm.loadData(start,end)\n",
    "    texts=tm.first_process(original_texts)\n",
    "    corpus, dictionary=tm.process_text(original_texts,texts)\n",
    "    \n",
    "    #run topic models\n",
    "    tm.runModels(number_of_topics,corpus, dictionary,start,end)\n",
    "    \n",
    "    #output coherence model\n",
    "    lmlist, c_v=tm.evaluate_graph(dictionary, corpus, original_texts, int(number_of_topics))\n",
    "    tm.printEvaluation(lmlist,c_v,number_of_topics,start,end)\n",
    "    \n",
    "    print('Finished')\n",
    "\n",
    "#run the module\n",
    "if __name__ == '__main__':\n",
    "    run(sys.argv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
