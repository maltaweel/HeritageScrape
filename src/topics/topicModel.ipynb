{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The main module used for topic modelling.\n",
    "Topic models include latent Dirichlet allocation (LDA) and Hierarchical Dirichlet Process (HDP).\n",
    "\n",
    "Created on Jul 1, 2020\n",
    "\n",
    "@author: mark\n",
    "'''\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "import datetime\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer as word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import CoherenceModel, LdaModel, HdpModel\n",
    "\n",
    "import re\n",
    "import pyLDAvis.gensim\n",
    "import gensim\n",
    "from gensim.utils import lemmatize\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "#use this as the starting pathway (the src folder)\n",
    "pn=os.path.abspath(\"\")\n",
    "pn=pn.split(\"src\")[0]\n",
    "\n",
    "#the world lemmatizer to lemmatize words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "class TopicModel():\n",
    "    \n",
    "    '''\n",
    "    Simple method to prepocess text using gensim\n",
    "    '''\n",
    "    def first_process(self,texts):\n",
    "\n",
    "        for t in texts:\n",
    "            t=yield gensim.utils.simple_preprocess(t, deacc=True, min_len=3)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Method to process text and lemmatize.\n",
    "    \n",
    "    @param fullt_text- the full text to create bigram\n",
    "    @param texts- texts to analyze for topic models and process using lemmatization.\n",
    "    \n",
    "    @return corpus- the corpus of text to analyze\n",
    "    @return dictionary- the term dictionary to match against\n",
    "    '''       \n",
    "    def process_text(self, full_text, texts):\n",
    "        \n",
    "        #develop bigram\n",
    "        bigram = gensim.models.Phrases(full_text, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "        bigram = gensim.models.phrases.Phraser(bigram)\n",
    " #      trigram = gensim.tsmodels.phrases.Phraser(full_text, threshold=100)\n",
    "        \n",
    "        #get texts from bigram\n",
    "        texts = [bigram[line] for line in texts]\n",
    "  #     texts = [[word.split('/')[0] for word in lemmatize(' '.join(line), allowed_tags=re.compile('(NN)'), min_length=3)] for line in texts]\n",
    "  #      texts_lemma = ' '.join([lemmatizer.lemmatize(w) for w in texts])\n",
    "        \n",
    "        #get lemmaztized words\n",
    "        texts_lemma=[]\n",
    "        for line in texts:\n",
    "            for word in line:\n",
    "                l=lemmatizer.lemmatize(word)\n",
    "                texts_lemma.append(l)\n",
    "        \n",
    "        #create dictionary\n",
    "        texts_lemma = [d.split() for d in texts_lemma]\n",
    "        dictionary = Dictionary(texts_lemma)\n",
    "        \n",
    "        #create the corpus\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts_lemma]\n",
    "        \n",
    "        return corpus, dictionary\n",
    "        \n",
    "    '''\n",
    "    Method to load text data based on a start and end data.\n",
    "    \n",
    "    @param start- the start date to load text\n",
    "    @param end- the end date to load text\n",
    "    '''  \n",
    "    def loadData(self,start,end):\n",
    "        \n",
    "        #the pathway to the data to analyze\n",
    "        directory=os.path.join(pn,'modified')\n",
    "        \n",
    "        rows=[]\n",
    "        \n",
    "        #get the text file from the directory\n",
    "        try:\n",
    "            for f in listdir(directory):\n",
    "                \n",
    "                \n",
    "                if '.csv' not in f:\n",
    "                    continue\n",
    "                \n",
    "                #open the text\n",
    "                with open(os.path.join(directory,f),'rt') as csvfile:\n",
    "                    \n",
    "                    #get the reader\n",
    "                    reader = csv.DictReader(csvfile)\n",
    "            \n",
    "                    #get single reader\n",
    "                    for row in reader:\n",
    "                        \n",
    "                        #get text\n",
    "                        text=row['Text'] #.decode('utf-8')\n",
    "                        \n",
    "                        #get date\n",
    "                        date=row['Datetime'].split(\" \")[0]\n",
    "                        \n",
    "                        #get dates (start and end)\n",
    "                        date_time_obj = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "                        start_d=datetime.datetime.strptime(start, '%Y-%m-%d')\n",
    "                        end_d=datetime.datetime.strptime(end, '%Y-%m-%d')\n",
    "                        \n",
    "                        date=date_time_obj.date()\n",
    "                        startD=start_d.date()\n",
    "                        endD=end_d.date()\n",
    "                        \n",
    "                        if date>=startD:\n",
    "                            if date<endD:\n",
    "                                rows.append(text)       \n",
    "                        \n",
    "        # the exception handling           \n",
    "        except IOError:\n",
    "            print (\"Could not read file:\", csvfile)               \n",
    "        \n",
    "        return rows\n",
    "    \n",
    "    \"\"\"\n",
    "    Method for using a coherence model to look at topic coherence for LDA models.\n",
    "    \n",
    "    @param dictionary- Gensim dictionary\n",
    "    @param corpus- Gensim corpus\n",
    "    @param limit- topic limit\n",
    "    \n",
    "    @return lm_list- List of LDA topic models\n",
    "    @return c_v- Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    def evaluate_graph(self, dictionary, corpus, texts, limit):\n",
    "    \n",
    "        c_v = []\n",
    "        lm_list = []\n",
    "        \n",
    "        #topic models made using LDA with the models analyzed using UMASS method coherence test\n",
    "        for num_topics in range(1, (limit*2)+1):\n",
    "            lm = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,alpha=\"auto\")\n",
    "            lm_list.append(lm)\n",
    "            cm = CoherenceModel(model=lm, texts=[texts], corpus=corpus, coherence='u_mass')\n",
    "            c_v.append(cm.get_coherence())\n",
    "            del cm\n",
    "            \n",
    "        return lm_list, c_v\n",
    "\n",
    "    '''\n",
    "    Method to print csv output results of the model coherence evaluations conducted. \n",
    "    \n",
    "    @param modList- the model evaluated\n",
    "    @param results- the result scores\n",
    "    @param i- the index output desired\n",
    "    @param start- the start date\n",
    "    @param end- the end date\n",
    "    '''\n",
    "    def printEvaluation(self,modList,results,i,start,end):\n",
    "        \n",
    "        #the file output pathway\n",
    "        filename=os.path.join(pn,'topic_model_results','evaluationTotal'+str(i)+\"_\"+start+\"_\"+end+\".csv\")   \n",
    "        \n",
    "        #outputs are the model (name) and score\n",
    "        fieldnames = ['Model','Score']\n",
    "        \n",
    "        #write to file\n",
    "        with open(filename, 'w') as csvf:\n",
    "                writer = csv.DictWriter(csvf, fieldnames=fieldnames)\n",
    "\n",
    "                writer.writeheader()\n",
    "                for t in range(0,len(modList)):\n",
    "                    #write output\n",
    "                    writer.writerow({'Model':str(modList[t]),'Score': str(results[t])})\n",
    "                \n",
    "    '''\n",
    "    Method to output LDA and HDP results of the analysis for a specific number of topics.\n",
    "    \n",
    "    @param i- the number of topics\n",
    "    @param results- the results from the model\n",
    "    @param model- the model used (e.g., lda, hdp) to output\n",
    "    @param start- the start date\n",
    "    @param end- the end date\n",
    "    '''\n",
    "    def printResults(self,i,results,model,start,end):\n",
    "        #output file and directory\n",
    "        filename=os.path.join(pn,'topic_model_results','analysis_results_'+model+str(i)+\"_\"+start+\"_\"+\n",
    "                              end+\".csv\")\n",
    "        \n",
    "        # the fieldnames or column headings\n",
    "        fieldnames = ['Topic','Term','Value']\n",
    "\n",
    "        with open(filename, 'w') as csvf:\n",
    "            writer = csv.DictWriter(csvf, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "            \n",
    "            #write from results\n",
    "            for l in results:\n",
    "                n=l[0]\n",
    "                v=l[1]\n",
    "                vvs=v.split(\"+\")\n",
    "                for vv in vvs:\n",
    "                    vvt=vv.split(\"*\")\n",
    "                    if len(vvt)<2:\n",
    "                        continue\n",
    "                    t=vvt[1]\n",
    "                    val=vvt[0]\n",
    "                    \n",
    "                    #write output\n",
    "                    writer.writerow({'Topic':str(n),'Term': str(t.encode(\"utf-8\")),'Value':str(val)})\n",
    "    \n",
    "    '''\n",
    "    Method to run the topic models (lda and hdp).\n",
    "    @param number_of_topics- the number of topics\n",
    "    @param corpus- the corpus of text\n",
    "    @param dictionary- the dictionary of terms\n",
    "    @param start- the start date\n",
    "    @param end- the end date\n",
    "    '''\n",
    "    def runModels(self,number_of_topics, corpus, dictionary,start,end):\n",
    "        \n",
    "        #do hdp model\n",
    "        hdpmodel = HdpModel(corpus=corpus, id2word=dictionary)\n",
    "        \n",
    "        hdpmodel.print_topics(num_topics=int(number_of_topics), num_words=10)\n",
    "        hdptopics = hdpmodel.show_topics(num_topics=int(number_of_topics))\n",
    "\n",
    "    #   result_dict=addTotalTermResults(hdptopics)\n",
    "            \n",
    "        #add results to total kept in a list     \n",
    "    #   addToResults(result_dict)\n",
    "    \n",
    "        #output results\n",
    "        self.printResults(number_of_topics,hdptopics,'hdp',start,end)\n",
    "        \n",
    "        #d lda model\n",
    "        ldamodel = LdaModel(corpus=corpus, num_topics=number_of_topics, id2word=dictionary,passes=20,iterations=400)\n",
    "       \n",
    "        ldamodel.save('lda'+number_of_topics+'.model')\n",
    "        ldatopics = ldamodel.show_topics(num_topics=int(number_of_topics))\n",
    "    \n",
    "    #   result_dict=addTotalTermResults(ldatopics)    \n",
    "    #   addToResults(result_dict)\n",
    "        self.printResults(number_of_topics,ldatopics,'lda',start,end)\n",
    "    \n",
    "    \n",
    "        visualisation = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "   \n",
    "        location=os.path.join(pn,'topic_model_results')\n",
    "     \n",
    "        #visualize outputs in html\n",
    "        pyLDAvis.save_html(visualisation, os.path.join(location,'LDA_Visualization'+str(number_of_topics)+\"_\"+start+\n",
    "                                                        \"_\"+end+'.html')) \n",
    "        \n",
    "        \n",
    "        \n",
    "'''\n",
    "Method to run the module.\n",
    "\n",
    "@param argv- the runtime argument\n",
    "'''           \n",
    "def run(argv):\n",
    "    \n",
    "    #get run arguments\n",
    "    tm=TopicModel()\n",
    "    number_of_topics = argv[1]\n",
    "    start=argv[2]\n",
    "    end=argv[3]\n",
    "    \n",
    "    #load and process text\n",
    "    original_texts=tm.loadData(start,end)\n",
    "    texts=tm.first_process(original_texts)\n",
    "    corpus, dictionary=tm.process_text(original_texts,texts)\n",
    "    \n",
    "    #run topic models\n",
    "    tm.runModels(number_of_topics,corpus, dictionary,start,end)\n",
    "    \n",
    "    #output coherence model\n",
    "    lmlist, c_v=tm.evaluate_graph(dictionary, corpus, original_texts, int(number_of_topics))\n",
    "    tm.printEvaluation(lmlist,c_v,number_of_topics,start,end)\n",
    "    \n",
    "    print('Finished')\n",
    "\n",
    "#run the module\n",
    "if __name__ == '__main__':\n",
    "    run(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/mark/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414/pyLDAvis-2.1.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /home/mark/miniconda3/lib/python3.7/site-packages (from pyLDAvis) (2.11.2)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /home/mark/miniconda3/lib/python3.7/site-packages (from pyLDAvis) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /home/mark/miniconda3/lib/python3.7/site-packages (from pyLDAvis) (1.18.4)\n",
      "Processing /home/mark/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb/funcy-1.14-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wheel>=0.23.0 in /home/mark/miniconda3/lib/python3.7/site-packages (from pyLDAvis) (0.34.2)\n",
      "Requirement already satisfied: future in /home/mark/miniconda3/lib/python3.7/site-packages (from pyLDAvis) (0.16.0)\n",
      "Collecting numexpr\n",
      "  Using cached numexpr-2.7.1-cp37-cp37m-manylinux1_x86_64.whl (162 kB)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /home/mark/miniconda3/lib/python3.7/site-packages (from pyLDAvis) (1.0.4)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /home/mark/miniconda3/lib/python3.7/site-packages (from pyLDAvis) (0.16.0)\n",
      "Requirement already satisfied: pytest in /home/mark/miniconda3/lib/python3.7/site-packages (from pyLDAvis) (5.4.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/mark/miniconda3/lib/python3.7/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/mark/miniconda3/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/mark/miniconda3/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /home/mark/miniconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (8.4.0)\n",
      "Requirement already satisfied: packaging in /home/mark/miniconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (20.4)\n",
      "Requirement already satisfied: wcwidth in /home/mark/miniconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.1.9)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /home/mark/miniconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.6.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /home/mark/miniconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.13.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/mark/miniconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (19.3.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /home/mark/miniconda3/lib/python3.7/site-packages (from pytest->pyLDAvis) (1.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/mark/miniconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.17.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/mark/miniconda3/lib/python3.7/site-packages (from packaging->pytest->pyLDAvis) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/mark/miniconda3/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->pyLDAvis) (3.1.0)\n",
      "\u001b[31mERROR: fastai 1.0.61 requires bottleneck, which is not installed.\u001b[0m\n",
      "Installing collected packages: funcy, numexpr, pyLDAvis\n",
      "Successfully installed funcy-1.14 numexpr-2.7.1 pyLDAvis-2.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
